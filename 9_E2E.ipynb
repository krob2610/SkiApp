{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from app.DataSpliter import GranularDataSplitter\n",
    "from app.transformers.smoother import Smoother\n",
    "from app.DataLoader import DataLoader\n",
    "from app.visualization_utils import draw_plotly\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't use snowplow data\n",
    "def read_files():\n",
    "    path = \"data/labeled/HUAWEI_MATE\"\n",
    "    files = os.listdir(path)\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\") and \"snow_plow\" not in file:\n",
    "            print(f'{path}/{str(file)}')\n",
    "            loader = DataLoader(f'{path}/{str(file)}')\n",
    "            dataframes.append(loader.load_transform_data())\n",
    "    return dataframes\n",
    "labeled_dataframes = read_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Smoothing and Grnularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_on = \"Orientation\"\n",
    "def split_data_granular(dataframe_list : list[pd.DataFrame], split_on :str):\n",
    "    orientation_dfs = []\n",
    "    for df in dataframe_list:\n",
    "        Granular_spliter = GranularDataSplitter(df)\n",
    "        Granular_spliter.split_into_granular()\n",
    "        orientation_dfs.append(Granular_spliter.granular_data[split_on].copy())\n",
    "    return orientation_dfs\n",
    "\n",
    "def smooth_data(dataframe_list : list[pd.DataFrame], sensor:str, window_size:int):\n",
    "    smoothed_dataframes = []\n",
    "    for df in dataframe_list:\n",
    "        smoother = Smoother(df)\n",
    "        smoothed_dataframes.append(smoother.smooth_data_backward_ma(window_size = window_size, columns= [sensor]))\n",
    "    return smoothed_dataframes\n",
    "\n",
    "orientation_dfs = split_data_granular(labeled_dataframes, split_on=split_on)\n",
    "smoothed_orientation_dfs = smooth_data(orientation_dfs, sensor=\"roll\", window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_orientation_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in orientation_dfs:\n",
    "    draw_plotly(data, \"Orientation\", granular=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in smoothed_orientation_dfs:\n",
    "    draw_plotly(data, \"Orientation\", granular=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pozbyc sie nanow z wygladzania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# () gradient descent with momentum? min max approach\n",
    "\n",
    "# Przykład wczytania danych do DataFrame\n",
    "# df = pd.read_csv('twoje_dane.csv')\n",
    "# Zakładam, że kolumna \"roll\" istnieje i index jest poprawny\n",
    "def plot_positions(series, positions, step, curves = None, find_maxima = None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(series, label='Series', color='blue')\n",
    "    plt.scatter(positions, series[positions], color='red', alpha=0.6, label='Positions')\n",
    "    if curves is not None:\n",
    "        plt.scatter(curves, series[curves], color='green', alpha=0.6,label='Curves')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    if find_maxima is None:\n",
    "        plt.title('Positions at step {}'.format(step))\n",
    "    else:\n",
    "        extremum = \"maxima\" if find_maxima else \"minima\"\n",
    "        plt.title('Positions at step {} for {}'.format(step, extremum))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_curve_points(df:pd.DataFrame, turn: Literal[\"L\", \"R\"] = None):\n",
    "    if turn:\n",
    "        return np.where(df[f'Curve'] == turn)[0].tolist()\n",
    "    return np.where(df[f'Curve'] != False)[0].tolist()\n",
    "    # Funkcja gradient descent do znajdowania ekstremów\n",
    "\n",
    "def gradient_descent(df, series, normalized_series, normalized_positions,  start_indices, learning_rate, steps, momentum, find_maxima, printing):\n",
    "    velocities = [0 for _ in start_indices]\n",
    "    for step in range(steps):\n",
    "        for i, idx in enumerate(normalized_positions):\n",
    "            ahead_idx = int(min(1, idx + 1/(len(series) - 1)) * (len(series) - 1))\n",
    "            behind_idx = int(max(0, idx - 1/(len(series) - 1)) * (len(series) - 1))\n",
    "\n",
    "            grad = (normalized_series[ahead_idx] - normalized_series[behind_idx]) / 2\n",
    "\n",
    "            # Update velocity and position with momentum\n",
    "            if find_maxima:\n",
    "                velocities[i] = momentum * velocities[i] + learning_rate * grad  # Gradient ascent for maxima\n",
    "            else:\n",
    "                velocities[i] = momentum * velocities[i] - learning_rate * grad  # Gradient descent for minima\n",
    "\n",
    "            new_idx = idx + velocities[i]\n",
    "            new_idx = max(0, min(1, new_idx))  # Ensure new_idx stays within 0-1\n",
    "\n",
    "            normalized_positions[i] = new_idx\n",
    "            #print for step for step\n",
    "            #print(f'point: {i}, ahed_idx: {ahead_idx}, behind_idx: {behind_idx}, grad: {grad}, new_idx: {new_idx}, velocities: {velocities[i]}')\n",
    "        positions = [int(idx * (len(series) - 1)) for idx in normalized_positions]\n",
    "        if step % 10 == 0 and printing:\n",
    "            plot_positions(series, positions, step, get_curve_points(df))\n",
    "    # print(set(history))\n",
    "    positions = [int(idx * (len(series) - 1)) for idx in normalized_positions]\n",
    "    plot_positions(series, positions, \"final\", get_curve_points(df), find_maxima)\n",
    "    return positions\n",
    "\n",
    "def gradient_descent_full(df, start_indices, learning_rate=0.01, steps=1000, momentum=0.98, printing = False):\n",
    "    series = df['roll'].values\n",
    "\n",
    "    positions = start_indices.copy()\n",
    "    #normalize series data to 0-1\n",
    "    normalized_series = (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "    # Normalize start_indices to 0-1\n",
    "    normalized_positions = [idx / (len(series) - 1) for idx in start_indices]\n",
    "\n",
    "    #initial plot\n",
    "    plot_positions(series, positions, \"beginning\", get_curve_points(df))\n",
    "\n",
    "    min_positions = gradient_descent(df, series, normalized_series, normalized_positions,  start_indices, learning_rate, steps, momentum, False, printing)\n",
    "    max_positions = gradient_descent(df, series, normalized_series, normalized_positions,  start_indices, learning_rate, steps, momentum, True, printing)\n",
    "\n",
    "    return min_positions, max_positions\n",
    "\n",
    "\n",
    "\n",
    "# Wybieramy kolumnę \"roll\" z DataFrame\n",
    "\n",
    "# Krok 1: Losowe wybieranie punktów początkowych\n",
    "#np.random.seed(42)  # Ustawienie ziarna losowości dla powtarzalności wyników\n",
    "num_points = 10\n",
    "start_indices =np.random.randint(1, len(smoothed_orientation_dfs[5]) , size=num_points)  #np.array([800])\n",
    "\n",
    "# Krok 2: Znalezienie ekstremów przy użyciu gradient descent\n",
    "final_positions_min, final_positions_maximums = gradient_descent_full(smoothed_orientation_dfs[5], start_indices, printing=False)\n",
    "\n",
    "# # Krok 3: Filtracja najważniejszych ekstremów\n",
    "# unique_positions, counts = np.unique(final_positions, return_counts=True)\n",
    "# threshold = np.percentile(counts, 1)  # Przykładowy próg\n",
    "# important_extrema_indices = unique_positions[counts >= threshold]\n",
    "\n",
    "# Wynik - DataFrame z najważniejszymi ekstremami\n",
    "#important_extrema_smoothed_roll = smoothed_roll.iloc[final_positions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Krok 3: Filtracja najważniejszych ekstremów\n",
    "\n",
    "def filter_extremes(final_positions):\n",
    "    unique_positions, counts = np.unique(final_positions, return_counts=True)\n",
    "    threshold = np.percentile(counts, 75)  # Przykładowy próg\n",
    "    important_extrema_indices = unique_positions[counts >= threshold]\n",
    "    return important_extrema_indices\n",
    "\n",
    "# unique_positions, counts = np.unique(final_positions_min, return_counts=True)\n",
    "# threshold = np.percentile(counts, 75)  # Przykładowy próg\n",
    "# important_extrema_indices = unique_positions[counts >= threshold]\n",
    "\n",
    "# Wynik - DataFrame z najważniejszymi ekstremami\n",
    "#important_extrema_smoothed_roll = smoothed_roll.iloc[final_positions]\n",
    "important_min = filter_extremes(final_positions_min)\n",
    "important_max = filter_extremes(final_positions_maximums)\n",
    "\n",
    "plot_positions(smoothed_orientation_dfs[5]['roll'].values, important_min, \"Selected min_point\", get_curve_points(smoothed_orientation_dfs[5], turn=\"L\"))\n",
    "\n",
    "plot_positions(smoothed_orientation_dfs[5]['roll'].values, important_max, \"Selected max_points\", get_curve_points(smoothed_orientation_dfs[5], turn=\"R\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_distance(df, positions, turn ):\n",
    "\n",
    "    curves = np.where(df['Curve'].isin([turn]))[0]\n",
    "    positions = np.sort(positions)\n",
    "    curves = np.sort(curves)\n",
    "\n",
    "    distances = []\n",
    "    val_diff = []\n",
    "\n",
    "    for i in range(len(positions)):\n",
    "        j = 0\n",
    "        while j < len(curves)-1 and abs(positions[i] - curves[j]) > abs(positions[i] - curves[j+1]):\n",
    "            j += 1\n",
    "        #distances.append(abs(positions[i] - curves[j]))\n",
    "        distances.append(positions[i] - curves[j])\n",
    "        val_diff.append(df['roll'].values[positions[i]] - df['roll'].values[curves[j]])\n",
    "    return distances, val_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_R, val_diff_R = get_distance(smoothed_orientation_dfs[5], important_max, \"R\")\n",
    "distances_L, val_diff_L = get_distance(smoothed_orientation_dfs[5], important_min, \"L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_diff_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_diff_R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ski_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
